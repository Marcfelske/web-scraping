{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3FsgxgJvqost"
   },
   "source": [
    "# Script to scrape websites provided a .csv with URLs\n",
    "\n",
    "## Special requirements for the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fy0rlvWMqtRN"
   },
   "source": [
    "# Install requirements via pip etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18273,
     "status": "ok",
     "timestamp": 1559463213427,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "bQfNqgJ8RbK2",
    "outputId": "1cdefd14-f143-4f93-ac94-bd958b3beef7"
   },
   "outputs": [],
   "source": [
    "run_in = 'local'\n",
    "#run_in = 'colab'\n",
    "\n",
    "if run_in == 'local':\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import bs4 as BeautifulSoup\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    from urllib.parse import urlparse\n",
    "    import pprint\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    import html2text as html2text\n",
    "    import datetime\n",
    "    import time\n",
    "    import re\n",
    "    from lxml import html\n",
    "    #from bs4 import BeautifulSoup\n",
    "    import json\n",
    "    \n",
    "    #TF-IDF stuff:\n",
    "    import string\n",
    "    import nltk\n",
    "    #nltk.download(\"popular\")\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "      \n",
    "if run_in == 'colab':\n",
    "    !pip install beautifulsoup4\n",
    "    !pip install requests\n",
    "    !pip install html2text  \n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import bs4 as BeautifulSoup\n",
    "    #from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import numpy as np\n",
    "    from urllib.parse import urlparse\n",
    "    from urllib.parse import urljoin\n",
    "    from urllib.parse import urlunsplit\n",
    "    from urllib.parse import urlsplit\n",
    "    import pprint\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    import html2text as html2text\n",
    "    import datetime\n",
    "    import time\n",
    "    import re\n",
    "    from lxml import html\n",
    "    import json\n",
    "    \n",
    "    #TF-IDF stuff:\n",
    "    import string\n",
    "    import nltk\n",
    "    nltk.download(\"popular\")\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGZxYYhjVNt4"
   },
   "source": [
    "Define base paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57563,
     "status": "ok",
     "timestamp": 1559463252731,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "WiVvtnXgVNAK",
    "outputId": "3af538d8-edb2-442c-fc4a-551aa6b1936b"
   },
   "outputs": [],
   "source": [
    "if run_in == 'local':\n",
    "  base_path = \"/users/USERNAME/Google Drive/USERNAME/\"\n",
    "if run_in == 'colab':\n",
    "  base_path = \"/content/drive/My Drive/Mark/\"\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  \n",
    "print('base_path set to:')\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJCxqjkCeWli"
   },
   "source": [
    "<p><font color=\"red\">Define Version number!</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhFrGNyZeVQy"
   },
   "outputs": [],
   "source": [
    "version_id = 'V14'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color=\"red\">Define whether first_time_run!</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time_run = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Q_HsIdUs8hm"
   },
   "source": [
    "#Read all start-ups and their website URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60784,
     "status": "ok",
     "timestamp": 1559463255966,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "zsKD6xhXs7kJ",
    "outputId": "b6961857-753f-448c-a54e-b623e3c71da1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(base_path + \"Pitchbook_Crunchbase_Tracxn_Raw/Don't touch \" + version_id + \"/\" + version_id + \".xlsx\", version_id + \"website_url_scrape\", header=None, skiprows=1, names=['company_name','domain'], usecols=\"A,B\")\n",
    "df = df.sort_values(by=['domain'])\n",
    "df = df.dropna()\n",
    "df['domain'] = df['domain'].str.lower()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"We have found \" + str(len(df)) + \" startups\")\n",
    "#df = df.loc[df['company_name'] == \"9fin\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqNhVXT8DOgJ"
   },
   "source": [
    "## Check for which startups we already have all the information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4e3gfX91DZFS"
   },
   "source": [
    "Startups for which we already have a website saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62669,
     "status": "ok",
     "timestamp": 1559463257883,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "LE0JpMfrOrAg",
    "outputId": "74cd751b-d22a-426c-e0a5-f06d5716507f"
   },
   "outputs": [],
   "source": [
    "website_contents_path = os.path.join(base_path + \"/outputs/Scraping Websites/Website Contents/\")\n",
    "manual_webscrape_contents_path = os.path.join(base_path + \"/outputs/Manual Webscrape/Raw/\")\n",
    "#os.listdir(website_contents_path)\n",
    "\n",
    "already_scraped_websites = [] # to be dropped once populated\n",
    "\n",
    "for root, dirs, files in os.walk(website_contents_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            #print(os.path.splitext(file)[0])\n",
    "            already_scraped_websites.append(os.path.splitext(file)[0])\n",
    "            \n",
    "for root, dirs, files in os.walk(manual_webscrape_contents_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            #print(os.path.splitext(file)[0])\n",
    "            already_scraped_websites.append(os.path.splitext(file)[0])\n",
    "            \n",
    "#print(already_scraped_websites)\n",
    "print(\"We already scraped the websites of\", str(len(already_scraped_websites)), \"startups.\")\n",
    "print(\"We still have to scrape the websites of\", str(len(df)-len(already_scraped_websites)), \"startups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAHTejToPpKu"
   },
   "source": [
    "Remove the already scraped websites from the dataframe of websites which are to be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62658,
     "status": "ok",
     "timestamp": 1559463257884,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "WNhYKnI_RY5t",
    "outputId": "4bc4df73-777d-4173-8600-04988f093b73"
   },
   "outputs": [],
   "source": [
    "df = df[~df['company_name'].isin(already_scraped_websites)]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62645,
     "status": "ok",
     "timestamp": 1559463257885,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "IAphBg_jCsmw",
    "outputId": "7644639b-8a2b-476d-db69-0e33c5bde225"
   },
   "outputs": [],
   "source": [
    "print(\"We will scrape URLs of \" + str(len(df)) + \" startups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.loc[df['domain'] != \"xrgenomics.com\"]\n",
    "#df = df.loc[df['company_name'] == \"reinfer\"]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_num_of_scrapes = 100\n",
    "df = df[-limit_num_of_scrapes:]\n",
    "print(\"We limited the number of websites to be scraped to\", limit_num_of_scrapes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jk2eyM-MD27B"
   },
   "source": [
    "Create company2domain dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62634,
     "status": "ok",
     "timestamp": 1559463257886,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "rj5byCm5bH0x",
    "outputId": "2987cb22-bdbc-4f83-89ae-5772cab9d91f"
   },
   "outputs": [],
   "source": [
    "company2domain = dict(zip(df.domain, df.company_name))\n",
    "print(company2domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(company2domain[\"techspert.io\"])\n",
    "#print(\"I think this should yield the company name Techspert...\")\n",
    "#print(\"I want to save the websites in a file with, startup, website\")\n",
    "#print(\"next have to change how the files are read in. They are read in by line but now have to be split and taken line[0]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(company2domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbB6czB72Bpq"
   },
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62621,
     "status": "ok",
     "timestamp": 1559463257889,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "2doyh6xS1AbN",
    "outputId": "e33ca2f8-e151-41b2-e2c1-7c57d6c3affa"
   },
   "outputs": [],
   "source": [
    "url_occurrences = df['domain'].value_counts()\n",
    "\n",
    "if url_occurrences.max() > 1:\n",
    "    print(url_occurrences[url_occurrences > 1])\n",
    "else:\n",
    "    print(\"There were no duplicate URLs for startups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62609,
     "status": "ok",
     "timestamp": 1559463257890,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "0MEnl8Zi4jTF",
    "outputId": "f06114d4-3948-457f-a7c5-403e2ce7c187"
   },
   "outputs": [],
   "source": [
    "list_of_domains = df['domain'].tolist()\n",
    "#print(list_of_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhqaCqbefV-p"
   },
   "source": [
    "# Define functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWHtvi_YE1Iu"
   },
   "source": [
    "Check if url responds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIetHmiRA9X3"
   },
   "outputs": [],
   "source": [
    "def try_url(url):\n",
    "  parsed_url = urlparse(url).geturl()\n",
    "  try:\n",
    "    result = requests.get(parsed_url)\n",
    "    if result.status_code == 200:\n",
    "      return(True)\n",
    "    else:\n",
    "      print(url,result.status_code)\n",
    "  except:\n",
    "    return(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DW0wWnGKE4YH"
   },
   "source": [
    "Check if url scheme is correct, if not, repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQXymqDQCGQ4"
   },
   "outputs": [],
   "source": [
    "def url_check(no_scheme_url):\n",
    "  \n",
    "  if no_scheme_url.startswith(\"www.\"):\n",
    "    print(\"no_scheme_url starts with www ->\", no_scheme_url)\n",
    "    no_scheme_url = no_scheme_url[4:]\n",
    "    print(\"no_scheme_url is now->\", no_scheme_url)\n",
    "    \n",
    "  if (\"http://\" not in no_scheme_url and \"https://\" not in no_scheme_url):\n",
    "    no_scheme_url = \"https://\" + no_scheme_url\n",
    "  \n",
    "  worked = try_url(no_scheme_url)\n",
    "\n",
    "  if not worked:\n",
    "    no_scheme_url = \"http://\" + no_scheme_url.split(\"//\")[1]\n",
    "    #print(no_scheme_url)\n",
    "\n",
    "    worked = try_url(no_scheme_url)\n",
    "    if not worked:\n",
    "      return (False, None)\n",
    "    else:\n",
    "      return(True, no_scheme_url)\n",
    "  \n",
    "  else:\n",
    "    return(True, no_scheme_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5SKdVJqDo_O"
   },
   "source": [
    "Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gspu6ZU_8Dqw"
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "   # 1 Take all types of hyphens apart from U+002D \n",
    "   de_hyphened_em = str.replace(text, \"—\", \" \")\n",
    "   de_hyphened_en = str.replace(de_hyphened_em, \"–\", \" \")\n",
    "   # 2 Tokenize\n",
    "   tokens = word_tokenize(de_hyphened_en)\n",
    "   tokens = [item.replace(\"—\", \" \") for item in tokens]\n",
    "   # 3 Remove puctuation\n",
    "   table = str.maketrans('', '', string.punctuation)\n",
    "   stripped = [w.translate(table) for w in tokens]\n",
    "   # Remove lonely letters\n",
    "   no_single = [w for w in stripped if len(w) > 1]\n",
    "   # 4 Lower Case\n",
    "   low_words = [word.lower() for word in no_single]\n",
    "   # 5 Not Alpha\n",
    "   no_alpha = [w for w in low_words if w.isalpha()]\n",
    "   # 6 Delete Stop Words\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "   no_stop = [w for w in no_alpha if not w in stop_words]\n",
    "   # 10 Stem Words\n",
    "   porter = PorterStemmer()\n",
    "   stemmed = [porter.stem(word) for word in no_stop] \n",
    "   return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST APPEND WRITING\n",
    "# list_of_broken_domains = [\"A\",\"D\"]\n",
    "# list_of_domains_test = [\"A\",\"B\",\"C\",\"D\"]\n",
    "\n",
    "# for url in list_of_domains_test:\n",
    "#     if url not in list_of_broken_domains:\n",
    "#         print(url)\n",
    "\n",
    "# with open(base_path + 'outputs/Scraping Websites/list_of_broken_domains.txt', 'a+') as f:\n",
    "#     for item in list_of_broken_domains:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FILE READING\n",
    "# with open(base_path + 'outputs/Scraping Websites/list_of_broken_domains.txt') as broken_domain_file:\n",
    "#   list_of_broken_domains = [word for line in broken_domain_file for word in line.split()]\n",
    "# list_of_broken_domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If run from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_time_run:\n",
    "    list_of_scrapable_domains = []\n",
    "    list_of_broken_domains = []\n",
    "\n",
    "    for url in list_of_domains[:]:\n",
    "      worked, correct_url = url_check(url)\n",
    "\n",
    "      if worked:\n",
    "        print(url, \"worked\")\n",
    "        list_of_scrapable_domains.append(correct_url)\n",
    "      else:\n",
    "        list_of_broken_domains.append(url)\n",
    "\n",
    "\n",
    "    print(\"works \", list_of_scrapable_domains)\n",
    "    print(\"\")\n",
    "    print(\"not \", list_of_broken_domains)\n",
    "\n",
    "    with open(base_path + 'outputs/Scraping Websites/list_of_broken_domains.txt', 'a+') as f:\n",
    "        for item in list_new_of_broken_domains:\n",
    "                company = company2domain[item]\n",
    "                problem_company = str(item) + \" | \" + str(company)\n",
    "                print(problem_company)\n",
    "                f.write(\"%s\\n\" % problem_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If broken links should be omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(base_path + 'outputs/Scraping Websites/list_of_broken_domains.txt') as broken_domain_file:\n",
    "#   list_of_broken_domains = []\n",
    "#   for line in broken_domain_file:\n",
    "#     #print(line)\n",
    "#     url = line.strip().split(' | ')[1]\n",
    "#     list_of_broken_domains.append(url)\n",
    "# print(list_of_broken_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 82443,
     "status": "ok",
     "timestamp": 1559463277740,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "apHrkD-igDCH",
    "outputId": "3661f48a-955e-49f3-9c98-2b9e1aef38c7"
   },
   "outputs": [],
   "source": [
    "if not first_time_run:\n",
    "    list_of_scrapable_domains = []\n",
    "    # obtain existing list_of_broken_domains from .txt file\n",
    "    with open(base_path + 'outputs/Scraping Websites/list_of_broken_domains.txt') as broken_domain_file:\n",
    "      list_new_of_broken_domains = []\n",
    "      list_of_broken_domains = []\n",
    "      for line in broken_domain_file:\n",
    "        #print(line)\n",
    "        url = line.strip().split(' | ')[1]\n",
    "        list_of_broken_domains.append(url)\n",
    "\n",
    "    for url in list_of_domains:\n",
    "      if url not in list_of_broken_domains:\n",
    "        #print(url)\n",
    "        worked, correct_url = url_check(url)\n",
    "\n",
    "        if worked:\n",
    "          list_of_scrapable_domains.append(correct_url)\n",
    "          print(\"SUCCESS!\", correct_url, \"saved to\",\"list_of_scrapable_domains\")\n",
    "        else:\n",
    "          list_new_of_broken_domains.append(url)\n",
    "          print(url, \"saved to\",\"list_of_broken_domains\")\n",
    "      if url in list_of_broken_domains:\n",
    "        print(url, \"was sorted out because it is known to be broken\")\n",
    "\n",
    "    # print(\"works \", list_of_scrapable_domains)\n",
    "    # print(\"\")\n",
    "    # print(\"not \", list_of_broken_domains)\n",
    "    with open(base_path + 'outputs/Scraping Websites/list_of_broken_domains.txt', 'a+') as f:\n",
    "        for item in list_new_of_broken_domains:\n",
    "            company = company2domain[item]\n",
    "            problem_company = str(item) + \" | \" + str(company)\n",
    "            print(problem_company)\n",
    "            f.write(\"%s\\n\" % problem_company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_broken_domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dKVA23gEEdi"
   },
   "source": [
    "##Obtain list of all working domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 82435,
     "status": "ok",
     "timestamp": 1559463277745,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "YV6z8BGDh_Ji",
    "outputId": "99dcb24a-17b6-4ce6-c9c5-3a71133563be"
   },
   "outputs": [],
   "source": [
    "print(\"We have found \" + str(len(list_of_scrapable_domains)) + \" urls where the main website responded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JusObveEVwT"
   },
   "source": [
    "##Find all sub websites on all main websites and save them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 24160
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 542740,
     "status": "ok",
     "timestamp": 1559463738063,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "KBYVHJvigDCO",
    "outputId": "28d205ee-79ae-4edc-a6d3-f19ed603208e"
   },
   "outputs": [],
   "source": [
    "list_of_results = []\n",
    "\n",
    "for k in list_of_scrapable_domains[:50]:\n",
    "  try:\n",
    "    print(k)\n",
    "    \n",
    "    if \"http://\" in k or \"https://\" in k:\n",
    "      domain = k.split(\"//\")[1]\n",
    "    else: domain = k\n",
    "    \n",
    "    netloc = urlparse(k).netloc\n",
    "    try:\n",
    "      company_name = company2domain[domain]\n",
    "      #print(\"company_name\", company_name)\n",
    "      #print(\"company2domain[domain]\", company2domain[domain])\n",
    "    except:  \n",
    "      company_name = \"error\"\n",
    "      print(\"this one had an error\", company_name, k)\n",
    "        \n",
    "    result = {\n",
    "        'domain': k,\n",
    "        'netloc': netloc,\n",
    "        'company_name': company_name,\n",
    "    }\n",
    "    \n",
    "    # create three lists which will contain the chosen and dropped urls\n",
    "    list_of_additionally_found_but_irrelevant_urls = []\n",
    "    list_of_additionally_found_urls = []\n",
    "    list_of_unusable_additionally_found_urls = []\n",
    "    \n",
    "    # obtain all links from the main website\n",
    "    r = requests.get(k)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup.BeautifulSoup(html_content, 'lxml') \n",
    "    temp_links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
    "    #print(\"\\033[1m\" + \"temp_links\" + \"\\033[0m\",temp_links)\n",
    "    \n",
    "    # delete all links which had '#', '/' or were duplicates\n",
    "    temp_links_no_endchar = set()\n",
    "    for t in temp_links:\n",
    "        if \"www.\" in t:\n",
    "            t = t.replace(\"www.\", \"\")\n",
    "        if t.endswith((('#', '/'))):\n",
    "            #print(\"t before\",t)\n",
    "            t = t[:-1]\n",
    "            #print(\"t after\",t)  \n",
    "        if t == \"\":\n",
    "            pass\n",
    "        else:\n",
    "            temp_links_no_endchar.add(t)\n",
    "    print(\"temp_links_no_endchar\", temp_links_no_endchar)\n",
    "\n",
    "    # check if all links have a domain and scheme\n",
    "    links = []\n",
    "    for link in temp_links_no_endchar:\n",
    "        #print(link)\n",
    "        if urlparse(link).scheme in ('http', 'https',):\n",
    "            #print(\"no action required\",link)\n",
    "            links.append(link)\n",
    "        else:\n",
    "            # check if the determined urls work and add to links list\n",
    "            worked, fixed_link = url_check(domain + link)\n",
    "            #print(\"fixed_link\", fixed_link)\n",
    "            links.append((fixed_link))\n",
    "    print(\"\\033[1m\" + \"links\" + \"\\033[0m\",links)\n",
    "    links = [x for x in links if x is not None]\n",
    "    \n",
    "    # add links to respective list\n",
    "    processed_links = []\n",
    "    for l in links:\n",
    "        print(\"processed_link\",l)\n",
    "        print(\"netloc is:\", netloc, \"urlparse is\", urlparse(l).netloc)\n",
    "        if netloc in urlparse(l).netloc:\n",
    "            processed_links.append(l)\n",
    "        else:\n",
    "            print(\"bad\", l)\n",
    "            list_of_additionally_found_but_irrelevant_urls.append(\"netloc not in url: \" + l)\n",
    "            \n",
    "    rule_outs = [\".pdf\",\".jpg\",\".jpeg\",\".png\",\"mailto\",\"twitter\"]\n",
    "    processed_links = [l for l in processed_links if not any(out in l for out in rule_outs)]\n",
    "    # add main page to the links in case it did not link to itself\n",
    "    processed_links.append(k)\n",
    "    print(\"processed_links\",processed_links)    \n",
    "\n",
    "    print(\"\\033[1m\" + \"No more duplicates in Links\" + \"\\033[0m\")\n",
    "    # check if all links work\n",
    "    for link in processed_links:\n",
    "        print(link)\n",
    "        worked, correct_link = url_check(link)\n",
    "        if worked:\n",
    "            list_of_additionally_found_urls.append(correct_link)\n",
    "        else:\n",
    "            list_of_unusable_additionally_found_urls.append((\"unsuccessful url check\",link))\n",
    "    \n",
    "    list_of_additionally_found_urls = list(set(list_of_additionally_found_urls))\n",
    "    #list_of_unusable_additionally_found_urls = list(set(list_of_unusable_additionally_found_urls))\n",
    "        \n",
    "    result['list_of_additionally_found_urls'] = list_of_additionally_found_urls\n",
    "    #result['list_of_unusable_additionally_found_urls']  = list_of_unusable_additionally_found_urls\n",
    "    result['list_of_additionally_found_but_irrelevant_urls'] = list_of_additionally_found_but_irrelevant_urls\n",
    "      \n",
    "    list_of_results.append(result)\n",
    "\n",
    "    #create a dataframe with all additionally_found_but_irrelevant_urls\n",
    "    list_company_name_for_df_of_additionally_found_but_irrelevant_urls = []\n",
    "    df_of_additionally_found_but_irrelevant_urls = pd.DataFrame()\n",
    "    for item in list_of_additionally_found_but_irrelevant_urls:\n",
    "      list_company_name_for_df_of_additionally_found_but_irrelevant_urls.append(company_name)\n",
    "      \n",
    "    df_of_additionally_found_but_irrelevant_urls[\"company_name\"] = list_company_name_for_df_of_additionally_found_but_irrelevant_urls\n",
    "    df_of_additionally_found_but_irrelevant_urls[\"domain\"] = list_of_additionally_found_but_irrelevant_urls\n",
    "    \n",
    "  except:\n",
    "    print(\"Exception next \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPDMs0blDsp4"
   },
   "source": [
    "##Iterate over all sub-websites and apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7024
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 542728,
     "status": "ok",
     "timestamp": 1559463738064,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "geuYt_m3gDCa",
    "outputId": "66131e47-2f85-498c-9e6b-02923032a2ae"
   },
   "outputs": [],
   "source": [
    "#print(len(list_of_results[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJpmakdNFG4e"
   },
   "source": [
    "##Scrape all websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8W6d0QYNUTka"
   },
   "outputs": [],
   "source": [
    "all_company_dict = {}\n",
    "list_of_accessible_but_unscrapable_domains = []\n",
    "\n",
    "for result in list_of_results:\n",
    "    try:\n",
    "        company_name = result[\"company_name\"]   \n",
    "        result[\"raw_web_content\"] = {}\n",
    "        result[\"stemmed_web_content\"] = {}\n",
    "        result[\"set_stemmed_web_content\"] = {}\n",
    "        list_stemmed_entire_web_content = []\n",
    "        result[\"set_stemmed_entire_web_content\"] = []\n",
    "\n",
    "        for res in result['list_of_additionally_found_urls'][:]:\n",
    "\n",
    "            print(res)\n",
    "            source = requests.get(res)\n",
    "            html_content = source.text\n",
    "            html_content = re.sub(\"<head>.*?</head>\", \"\", html_content, flags=re.DOTALL)\n",
    "            html_content = re.sub(\"<footer>.*?</footer>\", \"\", html_content, flags=re.DOTALL)\n",
    "            html_content = re.sub(\"<span.*?</span>\", \"\", html_content, flags=re.DOTALL)\n",
    "            raw_website_content = html2text.html2text(html_content)\n",
    "            raw_website_content = re.sub(r'\\([^)]*\\)', '', raw_website_content) # get rid of stuff in round brackets ( )\n",
    "            raw_website_content = re.sub(r'!?\\[[^\\]]*\\]', '', raw_website_content) # get rid of stuff in brackets []\n",
    "            raw_website_content = re.sub(r'<[^\\>]*>',' ',raw_website_content) # get rid of stuff in brackets <>\n",
    "            #raw_website_content = re.sub(r'\\d', '', raw_website_content) # get rid of all digits\n",
    "            raw_website_content = re.sub(r'[^a-zA-Z\\n\\s\\d\\.\\,\\!\\?\\'\\:]+', '', raw_website_content) # get rid of special characters apart from \\n space , . ! ? ' :\n",
    "            raw_website_content = re.sub(r'(?<!\\n)\\n{1,}', ' ', raw_website_content) # get rid of anything more than x newlines\n",
    "            raw_website_content = re.sub(' +',' ',raw_website_content) # get rid of everything more than one 'Space'\n",
    "            raw_website_content = raw_website_content.lstrip()\n",
    "\n",
    "            #all_company_dict[company_name][res] = raw_website_content\n",
    "            #company_dict[res] = raw_website_content\n",
    "            #pp.pprint(company_dict)\n",
    "            #put content of the raw sub-website in dictionary\n",
    "            result[\"raw_web_content\"][res] = raw_website_content\n",
    "\n",
    "            # stem the raw sub-website and put in dictionary\n",
    "            stemmed = text_cleaner(raw_website_content)\n",
    "            result[\"stemmed_web_content\"][res] = stemmed\n",
    "\n",
    "            # set of the stem dictionary for sub-website\n",
    "            set_stemmed = str(set(stemmed))\n",
    "            result[\"set_stemmed_web_content\"][res] = set_stemmed\n",
    "\n",
    "        for sub_webpage, sub_webpage_content in result[\"set_stemmed_web_content\"].items():\n",
    "            content = sub_webpage_content.replace('{', '').replace('}', '').replace(\"'\", \"\").replace(',', '')\n",
    "            content = content.split()\n",
    "            list_stemmed_entire_web_content.extend(content)\n",
    "        set_stemmed_entire_web_content = str(set(list_stemmed_entire_web_content))\n",
    "\n",
    "        result[\"set_stemmed_entire_web_content\"] = set_stemmed_entire_web_content\n",
    "\n",
    "        with open(website_contents_path + company_name + '.json', 'w') as website_contents:\n",
    "          json.dump(result, website_contents)\n",
    "          print(company_name, \"website saved\")\n",
    "    except:\n",
    "        print(\"Did not work\")\n",
    "        list_of_accessible_but_unscrapable_domains.append(res)\n",
    "        \n",
    "with open(base_path + 'outputs/Scraping Websites/list_of_accessible_but_unscrapable_domains.txt', 'a+') as f:\n",
    "    for item in list_of_accessible_but_unscrapable_domains:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQJ5yY6RFNSt"
   },
   "source": [
    "##Saved to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 764391,
     "status": "ok",
     "timestamp": 1559463959741,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "u-2Ampo3gpAz",
    "outputId": "35d9da8f-c2b3-4711-904d-2cc42499631d"
   },
   "outputs": [],
   "source": [
    "print(\"saved to:\", website_contents_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 2019-05-28 Beautifulsoup Problem otherwise working Web Scraper.ipynb",
   "provenance": [
    {
     "file_id": "1xljx-JBz67x7RJyie17LecgKgKS1msdr",
     "timestamp": 1559465801471
    },
    {
     "file_id": "1VmKkAUnKeaybgt781jATw-pKnd1X5Mqp",
     "timestamp": 1559039882526
    },
    {
     "file_id": "1Bl2mA5LrqMFqBNfc5vKY-OAy2Pj2XJPU",
     "timestamp": 1553787190799
    },
    {
     "file_id": "10Wx0ZsKSEVc0tet3tuq03PZ6vpddVAkd",
     "timestamp": 1553766524107
    },
    {
     "file_id": "1PZOEtVzT1FrWWkCVGb3SNp8_0GQtiR1x",
     "timestamp": 1553204863003
    },
    {
     "file_id": "1pDtpAigRurhRbh9goG3b0hxmVHeWj8ji",
     "timestamp": 1553122260090
    },
    {
     "file_id": "1So7L-02kX5x4U5GDH_0z-5CJPQ5Y5M9q",
     "timestamp": 1552471921615
    },
    {
     "file_id": "1tcKLyfIUKcnZyUU6S9gz5yB10PKEhRi2",
     "timestamp": 1552425666955
    },
    {
     "file_id": "1ndnlKhyrGpbBVLdYciV3oWY2jjM3LV1Q",
     "timestamp": 1552059916735
    },
    {
     "file_id": "1M4kNcYoAprVF3baUemodmE6_xBZfxcyJ",
     "timestamp": 1543338066712
    },
    {
     "file_id": "14zx_Z_Dr0bctdffrbq54mG1y4s5wFY2m",
     "timestamp": 1542796671668
    },
    {
     "file_id": "1KQt0THO6EA2fW0iQWHtF2oPCUuxUw70E",
     "timestamp": 1542744417484
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
